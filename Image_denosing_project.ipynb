{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvgen/Vlg-project/blob/main/Image_denosing_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d2afc181-8411-43a0-a09c-6401d8de62db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2afc181-8411-43a0-a09c-6401d8de62db",
        "outputId": "feda43f4-ee20-469a-96b0-bc78edb94620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current directory: /content/Train\n",
            "Current directory: /content/Train/Train\n",
            "'high' directory is located at: /content/Train/Train/high\n",
            "'low' directory is located at: /content/Train/Train/low\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Navigate to the current directory containing the notebook\n",
        "# Assuming the notebook is in '/content/drive/My Drive/Colab Notebooks'\n",
        "notebook_directory = '/content/Train'  # Adjust this path as needed\n",
        "\n",
        "'''with ZipFile(notebook_directory, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Train')'''\n",
        "\n",
        "# Change to the notebook directory\n",
        "os.chdir(notebook_directory)\n",
        "\n",
        "# Verify the current directory\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# Step 3: Define the path to the 'test' directory and navigate to it\n",
        "test_directory = 'Train'\n",
        "os.chdir(test_directory)\n",
        "\n",
        "# Verify the current directory\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# Step 4: Define the paths to the high and low quality directories\n",
        "high_quality_dir = 'high'\n",
        "low_quality_dir = 'low'\n",
        "\n",
        "# Verify the directories exist\n",
        "if os.path.exists(high_quality_dir) and os.path.exists(low_quality_dir):\n",
        "    print(f\"'high' directory is located at: {os.path.abspath(high_quality_dir)}\")\n",
        "    print(f\"'low' directory is located at: {os.path.abspath(low_quality_dir)}\")\n",
        "else:\n",
        "    print(\"One or both of the directories do not exist. Please check the paths.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4aeac9ea-c931-4b63-acb4-bf9b0392bfde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aeac9ea-c931-4b63-acb4-bf9b0392bfde",
        "outputId": "b7080497-3495-451a-b692-755d455987bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 485 high-quality images.\n",
            "Found 485 low-quality images.\n",
            "Successfully loaded 485 image pairs.\n"
          ]
        }
      ],
      "source": [
        "# Load image file paths\n",
        "high_quality_images = sorted(glob.glob(os.path.join(high_quality_dir, '*.png')))\n",
        "low_quality_images = sorted(glob.glob(os.path.join(low_quality_dir, '*.png')))\n",
        "\n",
        "# Ensure both directories contain the same number of images\n",
        "assert len(high_quality_images) == len(low_quality_images), \"The number of images in both directories must be the same.\"\n",
        "\n",
        "# Print the number of images found\n",
        "print(f\"Found {len(high_quality_images)} high-quality images.\")\n",
        "print(f\"Found {len(low_quality_images)} low-quality images.\")\n",
        "\n",
        "# Function to load and pair images\n",
        "def load_image_pairs(high_quality_images, low_quality_images):\n",
        "    pairs = []\n",
        "    for hq_img_path, lq_img_path in zip(high_quality_images, low_quality_images):\n",
        "        hq_img = cv2.imread(hq_img_path, cv2.IMREAD_COLOR)\n",
        "        lq_img = cv2.imread(lq_img_path, cv2.IMREAD_COLOR)\n",
        "        if hq_img is None or lq_img is None:\n",
        "            print(f\"Error reading images: {hq_img_path}, {lq_img_path}\")\n",
        "        else:\n",
        "            pairs.append((hq_img, lq_img))\n",
        "    return pairs\n",
        "\n",
        "# Load image pairs\n",
        "image_pairs = load_image_pairs(high_quality_images, low_quality_images)\n",
        "\n",
        "# Check if image pairs are loaded correctly\n",
        "if len(image_pairs) == 0:\n",
        "    print(\"No image pairs were loaded. Check your file paths and image formats.\")\n",
        "else:\n",
        "    print(f\"Successfully loaded {len(image_pairs)} image pairs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "92c0a45d-6a67-4515-96d8-8869a0ddd6c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92c0a45d-6a67-4515-96d8-8869a0ddd6c9",
        "outputId": "b07b359a-2e84-4175-cad1-d353a313aa23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First normalized pair shapes: (400, 600, 3), (400, 600, 3)\n",
            "Training set size: 291\n",
            "Validation set size: 97\n",
            "Test set size: 97\n"
          ]
        }
      ],
      "source": [
        "# Function to normalize images\n",
        "def normalize_image(image):\n",
        "    return image.astype(np.float32) / 255.0\n",
        "\n",
        "# Normalize all image pairs\n",
        "normalized_pairs = [(normalize_image(hq), normalize_image(lq)) for hq, lq in image_pairs]\n",
        "\n",
        "# Check normalization\n",
        "print(f\"First normalized pair shapes: {normalized_pairs[0][0].shape}, {normalized_pairs[0][1].shape}\")\n",
        "\n",
        "# Extract high-quality and low-quality images from pairs\n",
        "high_quality_images, low_quality_images = zip(*normalized_pairs)\n",
        "\n",
        "# Split the dataset\n",
        "hq_train, hq_test, lq_train, lq_test = train_test_split(\n",
        "    high_quality_images, low_quality_images, test_size=0.2, random_state=42)\n",
        "\n",
        "hq_train, hq_val, lq_train, lq_val = train_test_split(\n",
        "    hq_train, lq_train, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Convert lists to numpy arrays for easier processing later\n",
        "hq_train, hq_val, hq_test = np.array(hq_train), np.array(hq_val), np.array(hq_test)\n",
        "lq_train, lq_val, lq_test = np.array(lq_train), np.array(lq_val), np.array(lq_test)\n",
        "\n",
        "print(\"Training set size:\", len(hq_train))\n",
        "print(\"Validation set size:\", len(hq_val))\n",
        "print(\"Test set size:\", len(hq_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ec51a092-bbaa-4b14-8a6d-567bf8adccdb",
      "metadata": {
        "id": "ec51a092-bbaa-4b14-8a6d-567bf8adccdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom dataset class\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, hq_images, lq_images):\n",
        "        self.hq_images = hq_images\n",
        "        self.lq_images = lq_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hq_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hq_img = self.hq_images[idx]\n",
        "        lq_img = self.lq_images[idx]\n",
        "        hq_img = torch.from_numpy(hq_img.transpose((2, 0, 1)))\n",
        "        lq_img = torch.from_numpy(lq_img.transpose((2, 0, 1)))\n",
        "        return hq_img, lq_img\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "train_dataset = ImagePairDataset(hq_train, lq_train)\n",
        "val_dataset = ImagePairDataset(hq_val, lq_val)\n",
        "test_dataset = ImagePairDataset(hq_test, lq_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ece39b95-8ae9-4243-bee0-2b290927d4ab",
      "metadata": {
        "id": "ece39b95-8ae9-4243-bee0-2b290927d4ab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define your denoising model (example architecture)\n",
        "class DenoisingCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenoisingCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "753548f5-9a2b-4c1f-a51c-d92dde1d6f09",
      "metadata": {
        "id": "753548f5-9a2b-4c1f-a51c-d92dde1d6f09"
      },
      "outputs": [],
      "source": [
        "def initialize_model():\n",
        "    model = DenoisingCNN()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    return model, criterion, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6182dd20-9da1-4171-a596-dde848dabe9f",
      "metadata": {
        "id": "6182dd20-9da1-4171-a596-dde848dabe9f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, hq_images, lq_images):\n",
        "        self.hq_images = hq_images\n",
        "        self.lq_images = lq_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hq_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hq_img = self.hq_images[idx]\n",
        "        lq_img = self.lq_images[idx]\n",
        "        hq_img = torch.from_numpy(hq_img.transpose((2, 0, 1))).float()\n",
        "        lq_img = torch.from_numpy(lq_img.transpose((2, 0, 1))).float()\n",
        "        return hq_img, lq_img\n",
        "\n",
        "def create_dataloaders(hq_train, lq_train, hq_val, lq_val, batch_size=16):\n",
        "    train_dataset = ImagePairDataset(hq_train, lq_train)\n",
        "    val_dataset = ImagePairDataset(hq_val, lq_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "86a6cbd9-a382-4a16-90ba-997ae609f8a3",
      "metadata": {
        "id": "86a6cbd9-a382-4a16-90ba-997ae609f8a3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=3, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            print('It is working')\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs_val, targets_val in val_loader:\n",
        "                inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                outputs_val = model(inputs_val)\n",
        "                loss = criterion(outputs_val, targets_val)\n",
        "                val_loss += loss.item() * inputs_val.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f8954480-1bc9-4c1a-a5f8-b41a1ed89f8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8954480-1bc9-4c1a-a5f8-b41a1ed89f8e",
        "outputId": "9e76e3ed-72be-4217-bf3f-5469965c2a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "Epoch [1/2], Train Loss: 0.0039, Val Loss: 0.0025\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "It is working\n",
            "Epoch [2/2], Train Loss: 0.0035, Val Loss: 0.0028\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "# Assuming hq_train, lq_train, hq_val, and lq_val are defined numpy arrays of your images\n",
        "batch_size = 16\n",
        "train_loader, val_loader = create_dataloaders(hq_train, lq_train, hq_val, lq_val, batch_size=batch_size)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model, criterion, optimizer = initialize_model()\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=2)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(trained_model.state_dict(), '/content/denoising_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "6EQ6TZqrhjNV"
      },
      "id": "6EQ6TZqrhjNV"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6975aaa7-122e-4585-8bae-bdce65e06c8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6975aaa7-122e-4585-8bae-bdce65e06c8f",
        "outputId": "bcd00deb-c280-4aa6-d69d-475e53a89d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0039\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model /content/sample_data\n",
        "model = DenoisingCNN()\n",
        "model_path='/content/denoising_model.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs_test, targets_test in test_loader:\n",
        "        outputs_test = model(inputs_test.float())\n",
        "        loss = criterion(outputs_test, targets_test.float())\n",
        "        test_loss += loss.item() * inputs_test.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "84c2d7fd-8bf5-4b7b-ae07-eab5871398ed",
      "metadata": {
        "id": "84c2d7fd-8bf5-4b7b-ae07-eab5871398ed"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "def calculate_metrics(denoised, original):\n",
        "    psnr_value = peak_signal_noise_ratio(original, denoised)\n",
        "    # Adjust win_size to be smaller than the image size\n",
        "    ssim_value, _ = structural_similarity(original, denoised, win_size=5, full=True, multichannel=True)\n",
        "    return psnr_value, ssim_value\n",
        "\n",
        "psnr_values = []\n",
        "ssim_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (lq_tensor, original_img) in enumerate(test_loader):\n",
        "        denoised_tensor = model(lq_tensor.float())\n",
        "        denoised_img = denoised_tensor.squeeze().cpu().numpy()\n",
        "        original_img = original_img.squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage.transform"
      ],
      "metadata": {
        "id": "1ULYXuNwwopw"
      },
      "id": "1ULYXuNwwopw",
      "execution_count": 20,
      "outputs": []
    },
    {
      "source": [
        "# Resize the images to be larger than the win_size\n",
        "denoised_img = skimage.transform.resize(denoised_img, (500, 700))\n",
        "original_img = skimage.transform.resize(original_img, (500, 700))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZbM3q3l5v2hW"
      },
      "id": "ZbM3q3l5v2hW",
      "execution_count": 21,
      "outputs": []
    },
    {
      "source": [
        "# Adjust the win_size to be smaller than the size of the images\n",
        "psnr_value, ssim_value = calculate_metrics(denoised_img, original_img)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacdBQ-Hv4Gw",
        "outputId": "bf6ceb08-6831-4981-fc78-5ff1bfa78f2e"
      },
      "id": "LacdBQ-Hv4Gw",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e096ace01e85>:6: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_value, _ = structural_similarity(original, denoised, win_size=5, full=True, multichannel=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm1WHTqHxbsZ",
        "outputId": "a9ac0087-4b6e-48e9-8222-474bbde9afd1"
      },
      "id": "rm1WHTqHxbsZ",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.086864992181404"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}